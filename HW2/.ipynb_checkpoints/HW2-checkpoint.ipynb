{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/chenjieqing/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os,sys,re,csv\n",
    "import pickle\n",
    "from collections import Counter, defaultdict\n",
    "import numpy as np\n",
    "import scipy\n",
    "import math\n",
    "import random\n",
    "import nltk\n",
    "from scipy.spatial.distance import cosine\n",
    "from nltk.corpus import stopwords\n",
    "from numba import jit\n",
    "nltk.download('punkt')\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(10)\n",
    "np.random.seed(10)\n",
    "randcounter = 10\n",
    "np_randcounter = 10\n",
    "\n",
    "\n",
    "vocab_size = 0\n",
    "hidden_size = 100\n",
    "uniqueWords = [\"\"]                      #... list of all unique tokens\n",
    "wordcodes = {}                          #... dictionary mapping of words to indices in uniqueWords\n",
    "wordcounts = Counter()                  #... how many times each token occurs\n",
    "samplingTable = []                      #... table to draw negative samples from\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadData(filename):\n",
    "    global uniqueWords, wordcodes, wordcounts\n",
    "    override = False\n",
    "    if override:\n",
    "        #... for debugging purposes, reloading input file and tokenizing is quite slow\n",
    "        #...  >> simply reload the completed objects. Instantaneous.\n",
    "        fullrec = pickle.load(open(\"w2v_fullrec.p\",\"rb\"))\n",
    "        wordcodes = pickle.load( open(\"w2v_wordcodes.p\",\"rb\"))\n",
    "        uniqueWords= pickle.load(open(\"w2v_uniqueWords.p\",\"rb\"))\n",
    "        wordcounts = pickle.load(open(\"w2v_wordcounts.p\",\"rb\"))\n",
    "        return fullrec\n",
    "\n",
    "\n",
    "    # ... load in first 15,000 rows of unlabeled data file.  You can load in\n",
    "    # more if you want later (and should do this for the final homework)\n",
    "    handle = open(filename, \"r\", encoding=\"utf8\")\n",
    "    fullconts = handle.read().split(\"\\n\")\n",
    "    fullconts = fullconts[1:15000]  # (TASK) Use all the data for the final submission\n",
    "    #... apply simple tokenization (whitespace and lowercase)\n",
    "    fullconts = [\" \".join(fullconts).lower()]\n",
    "#     print(fullconts)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    print (\"Generating token stream...\")\n",
    "    #... (TASK) populate fullrec as one-dimension array of all tokens in the order they appear.\n",
    "    #... ignore stopwords in this process\n",
    "    #... for simplicity, you may use nltk.word_tokenize() to split fullconts.\n",
    "    #... keep track of the frequency counts of tokens in origcounts.\n",
    "    fullrec = nltk.word_tokenize(fullconts[0])\n",
    "#     print(fullrec)\n",
    "    min_count = 50\n",
    "    origcounts = Counter(fullrec)\n",
    "#     print(origcounts)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    print (\"Performing minimum thresholding..\")\n",
    "#     #... (TASK) populate array fullrec_filtered to include terms as-is that appeared at least min_count times\n",
    "#     #... replace other terms with <UNK> token.\n",
    "#     #... update frequency count of each token in dict wordcounts where: wordcounts[token] = freq(token)\n",
    "\n",
    "\n",
    "\n",
    "#     #... after filling in fullrec_filtered, replace the original fullrec with this one.\n",
    "    fullrec_filtered = ['<UNK>' if origcounts[word] < min_count else word for word in fullrec]\n",
    "    fullrec_filtered2 = []\n",
    "    for word in fullrec_filtered:\n",
    "        if word not in stopwords:\n",
    "            fullrec_filtered2.append(word)\n",
    "#     print(fullrec_filtered)\n",
    "    fullrec = fullrec_filtered2\n",
    "    wordcounts = Counter(fullrec)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    print (\"Producing one-hot indicies\")\n",
    "#     #... (TASK) sort the unique tokens into array uniqueWords\n",
    "#     #... produce their one-hot indices in dict wordcodes where wordcodes[token] = onehot_index(token)\n",
    "#     #... replace all word tokens in fullrec with their corresponding one-hot indices.\n",
    "    uniqueWords = sorted([word for word in wordcounts])#... fill in\n",
    "    \n",
    "#     print(uniqueWords)\n",
    "    wordcodes = {}\n",
    "    for word in uniqueWords:\n",
    "#         one_hot = np.zeros(len(uniqueWords),)\n",
    "#         wordIndex = uniqueWords.index(word)\n",
    "#         one_hot[wordIndex] = 1\n",
    "#         wordcodes[word] = one_hot\n",
    "        wordcodes[word] = uniqueWords.index(word)\n",
    "#     print(wordcodes)\n",
    "#     wordcodes = #... fill in\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     #... close input file handle\n",
    "    handle.close()\n",
    "\n",
    "\n",
    "\n",
    "#     #... store these objects for later.\n",
    "#     #... for debugging, don't keep re-tokenizing same data in same way.\n",
    "#     #... just reload the already-processed input data with pickles.\n",
    "#     #... NOTE: you have to reload data from scratch if you change the min_count, tokenization or number of input rows\n",
    "\n",
    "    pickle.dump(fullrec, open(\"w2v_fullrec.p\",\"wb+\"))\n",
    "    pickle.dump(wordcodes, open(\"w2v_wordcodes.p\",\"wb+\"))\n",
    "    pickle.dump(uniqueWords, open(\"w2v_uniqueWords.p\",\"wb+\"))\n",
    "    pickle.dump(dict(wordcounts), open(\"w2v_wordcounts.p\",\"wb+\"))\n",
    "\n",
    "\n",
    "#     #... output fullrec should be sequence of tokens, each represented as their one-hot index from wordcodes.\n",
    "    fullrec  = [wordcodes[word] for word in fullrec]\n",
    "#     print(fullrec)\n",
    "    return fullrec\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def sigmoid(x):\n",
    "    return 1.0/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negativeSampleTable(train_data, uniqueWords, wordcounts, exp_power=0.75):\n",
    "#     global wordcounts\n",
    "    #... stores the normalizing denominator (count of all tokens, each count raised to exp_power)\n",
    "    max_exp_count = 0\n",
    "    table_size = 1e7\n",
    "#     print(train_data)\n",
    "    print (\"Generating exponentiated count vectors\")\n",
    "    #... (TASK) for each uniqueWord, compute the frequency of that word to the power of exp_power\n",
    "    #... store results in exp_count_array.\n",
    "    exp_count_array = []\n",
    "    for word in uniqueWords:\n",
    "        if word != \"<UNK>\":\n",
    "            exp_count_array.append(wordcounts[word]**exp_power) #... fill in\n",
    "#     print(exp_count_array)\n",
    "    max_exp_count = sum(exp_count_array)\n",
    "    \n",
    "\n",
    "    print (\"Generating distribution\")\n",
    "\n",
    "    #... (TASK) compute the normalized probabilities of each term.\n",
    "    #... using exp_count_array, normalize each value by the total value max_exp_count so that\n",
    "    #... they all add up to 1. Store this corresponding array in prob_dist\n",
    "    prob_dist = [count_freq / max_exp_count for count_freq in exp_count_array] #... fill in\n",
    "#     print(prob_dist)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    print (\"Filling up sampling table\")\n",
    "    #... (TASK) create a dict of size table_size where each key is a sequential number and its value is a one-hot index\n",
    "    #... the number of sequential keys containing the same one-hot index should be proportional to its prob_dist value\n",
    "    #... multiplied by table_size. This table should be stored in cumulative_dict.\n",
    "    #... we do this for much faster lookup later on when sampling from this table.\n",
    "    table_size = 1e7\n",
    "    cumulative_dict = {}#... fill in\n",
    "    \n",
    "#     freqTempList = [freq*table_size for freq in prob_dist] \n",
    "# #     keyTempList = [i for i in range(table_size)]\n",
    "#     valueTempList = []\n",
    "#     for i,freq in freqTempList:\n",
    "#         for j in freq:\n",
    "#             valueTempList.append(train_data[i])\n",
    "    begin = 0\n",
    "    for i,prob in enumerate(prob_dist):\n",
    "        freq = int(round(prob*table_size))\n",
    "        for j in range(freq):\n",
    "            cumulative_dict[j+begin] = i\n",
    "        begin += freq\n",
    "    \n",
    "        \n",
    "#     cumulative_dict = dict(zip(keyTempList,valueTempList))\n",
    "            \n",
    "            \n",
    "        \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    return cumulative_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateSamples(context_idx, num_samples):\n",
    "    global samplingTable, uniqueWords, randcounter\n",
    "    results = []\n",
    "    #... (TASK) randomly sample num_samples token indices from samplingTable.\n",
    "    #... don't allow the chosen token to be context_idx.\n",
    "    #... append the chosen indices to results\n",
    "    while len(results) < num_samples:\n",
    "        sample = samplingTable[random.randint(0, len(samplingTable)-1)]\n",
    "#         while samplingTable[random.randint(0, len(samplingTable)-1)] = context_idx:\n",
    "#             sample = samplingTable[random.randint(0, len(samplingTable)-1)]\n",
    "#         results.append(sample)\n",
    "        if sample != context_idx:\n",
    "            results.append(sample)\n",
    "\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit(nopython=True)\n",
    "def performDescent(learning_rate, center_token, context_index,W1,W2,negative_indices):# delete the sequence_chars and num_samples\n",
    "    # sequence chars was generated from the mapped sequence in the core code\n",
    "    nll_new = 0\n",
    "#     for k in range(0, len(sequence_chars)):\n",
    "        #... (TASK) implement gradient descent. Find the current context token from sequence_chars\n",
    "        #... and the associated negative samples from negative_indices. Run gradient descent on both\n",
    "        #... weight matrices W1 and W2.\n",
    "        #... compute the total negative log-likelihood and store this in nll_new.\n",
    "    \n",
    "    H = W1[center_token].T\n",
    "    y_predict = sigmoid((W2[:,context_index].T.dot(H)))\n",
    "    error = y_predict - 1\n",
    "    \n",
    "    total = error*W2[:,context_index]\n",
    "    W2[:,context_index] = W2[:,context_index] - learning_rate*error*H\n",
    "    nll_new += -np.log(sigmoid((W2[:,context_index].T.dot(H))))\n",
    "    \n",
    "    for index in negative_indices:\n",
    "        y_predict_negative = sigmoid((W2[:,index].T.dot(H)))\n",
    "        error_negative = y_predict_negative - 0\n",
    "        \n",
    "        total += error_negative*W2[:,index]\n",
    "        W2[:,index] = W2[:,index] - learning_rate*error_negative*H \n",
    "        \n",
    "        nll_new += -np.log(sigmoid((-W2[:,index].T.dot(H))))\n",
    "        \n",
    "    W1[center_token] = W1[center_token] - learning_rate*total.T\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    return nll_new\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(curW1 = None, curW2=None):\n",
    "    global uniqueWords, wordcodes, fullsequence, vocab_size, hidden_size,np_randcounter, randcounter\n",
    "    vocab_size = len(uniqueWords)           #... unique characters\n",
    "    hidden_size = 100                       #... number of hidden neurons\n",
    "    context_window = [-2,-1,1,2]            #... specifies which context indices are output. Indices relative to target word. Don't include index 0 itself.\n",
    "    nll_results = []                        #... keep array of negative log-likelihood after every 1000 iterations\n",
    "\n",
    "\n",
    "    #... determine how much of the full sequence we can use while still accommodating the context window\n",
    "    start_point = int(math.fabs(min(context_window)))\n",
    "    end_point = len(fullsequence)-(max(max(context_window),0))\n",
    "    mapped_sequence = fullsequence\n",
    "\n",
    "\n",
    "\n",
    "    #... initialize the weight matrices. W1 is from input->hidden and W2 is from hidden->output.\n",
    "    if curW1==None:\n",
    "        np_randcounter += 1\n",
    "        W1 = np.random.uniform(-.5, .5, size=(vocab_size, hidden_size))\n",
    "        W2 = np.random.uniform(-.5, .5, size=(hidden_size, vocab_size))\n",
    "    else:\n",
    "        #... initialized from pre-loaded file\n",
    "        W1 = curW1\n",
    "        W2 = curW2\n",
    "\n",
    "\n",
    "\n",
    "    #... set the training parameters\n",
    "    epochs = 5\n",
    "    num_samples = 2\n",
    "    learning_rate = 0.2\n",
    "    nll = 0\n",
    "    iternum = 0\n",
    "    \n",
    "\n",
    "\n",
    "    indexUNK = wordcodes['<UNK>']\n",
    "    #... Begin actual training\n",
    "    for j in range(0,epochs):\n",
    "        print (\"Epoch: \", j)\n",
    "        prevmark = 0\n",
    "\n",
    "        #... For each epoch, redo the whole sequence...\n",
    "        for i in range(start_point,end_point):\n",
    "            iternum += 1\n",
    "\n",
    "            if (float(i)/len(mapped_sequence))>=(prevmark+0.1):\n",
    "                print (\"Progress: \", round(prevmark+0.1,1))\n",
    "                prevmark += 0.1\n",
    "            if iternum%10000==0:\n",
    "                print (\"Negative likelihood: \", nll)\n",
    "                nll_results.append(nll)\n",
    "                nll = 0\n",
    "\n",
    "\n",
    "            #... (TASK) determine which token is our current input. Remember that we're looping through mapped_sequence\n",
    "            if mapped_sequence[i] != indexUNK :\n",
    "                center_token = mapped_sequence[i]#... fill in\n",
    "            #... (TASK) don't allow the center_token to be <UNK>. move to next iteration if you found <UNK>.\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "\n",
    "#             iternum += 1\n",
    "            #... now propagate to each of the context outputs\n",
    "            for k in range(0, len(context_window)):\n",
    "\n",
    "                #... (TASK) Use context_window to find one-hot index of the current context token.\n",
    "                context_index = mapped_sequence[context_window[k] + i]#... fill in\n",
    "\n",
    "\n",
    "\n",
    "                #... construct some negative samples\n",
    "                negative_indices = generateSamples(context_index, num_samples)\n",
    "\n",
    "                #... (TASK) You have your context token and your negative samples.\n",
    "                #... Perform gradient descent on both weight matrices.\n",
    "                #... Also keep track of the negative log-likelihood in variable nll.\n",
    "                nll = performDescent(learning_rate, center_token, context_index,W1,W2,negative_indices)\n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    for nll_res in nll_results:\n",
    "        print (nll_res)\n",
    "    return [W1,W2]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating token stream...\n",
      "Performing minimum thresholding..\n",
      "Producing one-hot indicies\n",
      "Full sequence loaded...\n",
      "Total unique words:  1209\n",
      "Preparing negative sampling table\n",
      "Generating exponentiated count vectors\n",
      "Generating distribution\n",
      "Filling up sampling table\n",
      "Epoch:  0\n",
      "Negative likelihood:  0.7255216852425433\n",
      "Negative likelihood:  0.9385353714531874\n",
      "Negative likelihood:  0.11255061580097799\n",
      "Negative likelihood:  0.2384316566980278\n",
      "Progress:  0.1\n",
      "Negative likelihood:  0.1292066705179405\n",
      "Negative likelihood:  1.261214269347479\n",
      "Negative likelihood:  1.0984464143186004\n",
      "Negative likelihood:  0.34086700048740315\n",
      "Progress:  0.2\n",
      "Negative likelihood:  0.18495569692617528\n",
      "Negative likelihood:  0.0980116891800537\n",
      "Negative likelihood:  0.1095225206954143\n",
      "Negative likelihood:  3.658811707438858\n",
      "Negative likelihood:  0.19078432609048473\n",
      "Progress:  0.3\n",
      "Negative likelihood:  0.1374907231458898\n",
      "Negative likelihood:  0.052483660809480887\n",
      "Negative likelihood:  0.021056963482272203\n",
      "Negative likelihood:  2.9247461773795655\n",
      "Progress:  0.4\n",
      "Negative likelihood:  0.23300294187736514\n",
      "Negative likelihood:  0.1909766671132867\n",
      "Negative likelihood:  0.012465978019071728\n",
      "Negative likelihood:  0.36885605481312633\n",
      "Negative likelihood:  0.1625182398932596\n",
      "Progress:  0.5\n",
      "Negative likelihood:  0.09182195158824996\n",
      "Negative likelihood:  0.06513916697371393\n",
      "Negative likelihood:  0.3755330922990082\n",
      "Negative likelihood:  0.07269653615793759\n",
      "Progress:  0.6\n",
      "Negative likelihood:  0.11194905306947256\n",
      "Negative likelihood:  0.5131329541477632\n",
      "Negative likelihood:  0.23200857754457258\n",
      "Negative likelihood:  0.7625335521040016\n",
      "Progress:  0.7\n",
      "Negative likelihood:  0.4607718946708069\n",
      "Negative likelihood:  0.5295903057795278\n",
      "Negative likelihood:  0.005210401477348923\n",
      "Negative likelihood:  0.08064290527586293\n",
      "Negative likelihood:  0.03961156224562681\n",
      "Progress:  0.8\n",
      "Negative likelihood:  0.4368650366767031\n",
      "Negative likelihood:  0.9600350849297484\n",
      "Negative likelihood:  0.5422878855237417\n",
      "Negative likelihood:  0.0006689072595732488\n",
      "Progress:  0.9\n",
      "Negative likelihood:  0.0071609116209105095\n",
      "Negative likelihood:  0.9422069031813309\n",
      "Negative likelihood:  0.08212516969867428\n",
      "Negative likelihood:  0.33685288286403986\n",
      "Negative likelihood:  0.08236035614847723\n",
      "Epoch:  1\n",
      "Negative likelihood:  0.8231840666641862\n",
      "Negative likelihood:  0.3510304575179755\n",
      "Negative likelihood:  0.031349852415914085\n",
      "Negative likelihood:  0.06731776221506093\n",
      "Progress:  0.1\n",
      "Negative likelihood:  0.1110168009674357\n",
      "Negative likelihood:  0.1397448915146664\n",
      "Negative likelihood:  0.4549070812824013\n",
      "Negative likelihood:  0.25836198006282035\n",
      "Negative likelihood:  0.8447217478458272\n",
      "Progress:  0.2\n",
      "Negative likelihood:  0.0013376059908810894\n",
      "Negative likelihood:  0.1469319544221567\n",
      "Negative likelihood:  0.018308640581516118\n",
      "Negative likelihood:  0.17309599898317304\n",
      "Progress:  0.3\n",
      "Negative likelihood:  0.00522818234275803\n",
      "Negative likelihood:  0.20398104573470302\n",
      "Negative likelihood:  0.19139545164400296\n",
      "Negative likelihood:  0.1761040625613156\n",
      "Progress:  0.4\n",
      "Negative likelihood:  0.06828914102985914\n",
      "Negative likelihood:  0.36694036217199655\n",
      "Negative likelihood:  0.2680093120636835\n",
      "Negative likelihood:  0.0929836656171197\n",
      "Negative likelihood:  0.08359270596693691\n",
      "Progress:  0.5\n",
      "Negative likelihood:  0.1768738175009167\n",
      "Negative likelihood:  0.005313196999807807\n",
      "Negative likelihood:  0.13587372982608129\n",
      "Negative likelihood:  0.24748840711070508\n",
      "Progress:  0.6\n",
      "Negative likelihood:  0.05349103247145902\n",
      "Negative likelihood:  0.00785934956485844\n",
      "Negative likelihood:  0.17672319454497443\n",
      "Negative likelihood:  0.16654628525205878\n",
      "Negative likelihood:  0.08223692635583114\n",
      "Progress:  0.7\n",
      "Negative likelihood:  0.3306304324415533\n",
      "Negative likelihood:  0.007486982032274599\n",
      "Negative likelihood:  0.4898379116097532\n",
      "Negative likelihood:  0.10751942823229403\n",
      "Progress:  0.8\n",
      "Negative likelihood:  0.12738402483194566\n",
      "Negative likelihood:  0.1447592059806304\n",
      "Negative likelihood:  0.01258327055744058\n",
      "Negative likelihood:  0.32404438762852794\n",
      "Progress:  0.9\n",
      "Negative likelihood:  0.061634150131906554\n",
      "Negative likelihood:  0.15883952459213674\n",
      "Negative likelihood:  2.1555076726413045\n",
      "Negative likelihood:  0.4542203864128671\n",
      "Negative likelihood:  0.17123060314281394\n",
      "Epoch:  2\n",
      "Negative likelihood:  0.18680166171671123\n",
      "Negative likelihood:  0.02174178746317127\n",
      "Negative likelihood:  0.13139218451680904\n",
      "Negative likelihood:  0.22836824050900137\n",
      "Progress:  0.1\n",
      "Negative likelihood:  0.014338449903351874\n",
      "Negative likelihood:  0.08175761652676992\n",
      "Negative likelihood:  0.005026458059681172\n",
      "Negative likelihood:  0.08822497260573343\n",
      "Negative likelihood:  0.22463957626682418\n",
      "Progress:  0.2\n",
      "Negative likelihood:  0.12565589663308332\n",
      "Negative likelihood:  0.1767793520914916\n",
      "Negative likelihood:  0.2632275159866485\n",
      "Negative likelihood:  0.2327330947387958\n",
      "Progress:  0.3\n",
      "Negative likelihood:  0.23384644839626836\n",
      "Negative likelihood:  0.31473074939819495\n",
      "Negative likelihood:  0.5081574306271166\n",
      "Negative likelihood:  0.5596042800203863\n",
      "Negative likelihood:  0.3061412320811103\n",
      "Progress:  0.4\n",
      "Negative likelihood:  4.3495972413334885\n",
      "Negative likelihood:  0.001963168361808392\n",
      "Negative likelihood:  0.2986390916115732\n",
      "Negative likelihood:  0.000527117702255617\n",
      "Progress:  0.5\n",
      "Negative likelihood:  0.7845514256152308\n",
      "Negative likelihood:  0.24588641001834585\n",
      "Negative likelihood:  4.168505504593249\n",
      "Negative likelihood:  0.013882865872110014\n",
      "Progress:  0.6\n",
      "Negative likelihood:  1.0255194173115834\n",
      "Negative likelihood:  0.21206785939870504\n",
      "Negative likelihood:  0.041912209748812476\n",
      "Negative likelihood:  0.07651294642919731\n",
      "Negative likelihood:  0.24142821816699186\n",
      "Progress:  0.7\n",
      "Negative likelihood:  0.0012268588163174133\n",
      "Negative likelihood:  0.12126518690178098\n",
      "Negative likelihood:  0.7510809107599956\n",
      "Negative likelihood:  0.22328933707751208\n",
      "Progress:  0.8\n",
      "Negative likelihood:  0.01102651403042977\n",
      "Negative likelihood:  0.23724541596624754\n",
      "Negative likelihood:  1.1167708383161854\n",
      "Negative likelihood:  0.30372778191993044\n",
      "Negative likelihood:  0.2000322366199741\n",
      "Progress:  0.9\n",
      "Negative likelihood:  5.127006362746008\n",
      "Negative likelihood:  1.120641046878545\n",
      "Negative likelihood:  0.08025587786106991\n",
      "Negative likelihood:  0.1288968017450577\n",
      "Epoch:  3\n",
      "Negative likelihood:  0.09285218797851974\n",
      "Negative likelihood:  0.14699075933264225\n",
      "Negative likelihood:  0.2601323074878952\n",
      "Negative likelihood:  0.0009196477499673611\n",
      "Negative likelihood:  0.7126399505047462\n",
      "Progress:  0.1\n",
      "Negative likelihood:  0.15634857582370326\n",
      "Negative likelihood:  1.1763062995631832\n",
      "Negative likelihood:  0.04699280994404169\n",
      "Negative likelihood:  0.12489730384375883\n",
      "Progress:  0.2\n",
      "Negative likelihood:  0.3874670539533692\n",
      "Negative likelihood:  0.2920796668652863\n",
      "Negative likelihood:  0.052996015975586896\n",
      "Negative likelihood:  0.2910772799742567\n",
      "Progress:  0.3\n",
      "Negative likelihood:  0.20218393670814078\n",
      "Negative likelihood:  0.048656919469159766\n",
      "Negative likelihood:  0.00011307114005696693\n",
      "Negative likelihood:  0.037768725538756306\n",
      "Negative likelihood:  0.18408011309119562\n",
      "Progress:  0.4\n",
      "Negative likelihood:  0.5249141245664135\n",
      "Negative likelihood:  0.23635174010249257\n",
      "Negative likelihood:  0.17223215970212313\n",
      "Negative likelihood:  0.003001834675235188\n",
      "Progress:  0.5\n",
      "Negative likelihood:  0.2820343574528612\n",
      "Negative likelihood:  0.12441794890253896\n",
      "Negative likelihood:  0.14448078004027368\n",
      "Negative likelihood:  0.3852346155535776\n",
      "Negative likelihood:  0.121445524925186\n",
      "Progress:  0.6\n",
      "Negative likelihood:  0.2663197389377264\n",
      "Negative likelihood:  0.026244139327631255\n",
      "Negative likelihood:  1.300822002852302\n",
      "Negative likelihood:  0.17775827969485372\n",
      "Progress:  0.7\n",
      "Negative likelihood:  0.20746887631524963\n",
      "Negative likelihood:  0.14775987636336413\n",
      "Negative likelihood:  0.1916841814357523\n",
      "Negative likelihood:  0.2633671356114425\n",
      "Progress:  0.8\n",
      "Negative likelihood:  0.1466691158788942\n",
      "Negative likelihood:  0.04483602950177674\n",
      "Negative likelihood:  0.2962023568872615\n",
      "Negative likelihood:  0.03653763000246827\n",
      "Negative likelihood:  0.11194256687072834\n",
      "Progress:  0.9\n",
      "Negative likelihood:  0.3706410511281561\n",
      "Negative likelihood:  0.1403483496086487\n",
      "Negative likelihood:  0.0834407664445401\n",
      "Negative likelihood:  0.02066522941453643\n",
      "Epoch:  4\n",
      "Negative likelihood:  0.010666568865080338\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative likelihood:  0.6331558833107264\n",
      "Negative likelihood:  0.32276347837278796\n",
      "Negative likelihood:  0.40296738117409775\n",
      "Negative likelihood:  0.3639505709179785\n",
      "Progress:  0.1\n",
      "Negative likelihood:  0.8906951710813692\n",
      "Negative likelihood:  0.19525332925768685\n",
      "Negative likelihood:  0.10836071479182588\n",
      "Negative likelihood:  0.1589062715833177\n",
      "Progress:  0.2\n",
      "Negative likelihood:  0.21789490466126987\n",
      "Negative likelihood:  0.05332448732953174\n",
      "Negative likelihood:  0.12140825110546985\n",
      "Negative likelihood:  0.1594688284791159\n",
      "Negative likelihood:  0.09415802404490896\n",
      "Progress:  0.3\n",
      "Negative likelihood:  0.001393299670375245\n",
      "Negative likelihood:  0.021149378843142455\n",
      "Negative likelihood:  0.3128507716275005\n",
      "Negative likelihood:  0.08780611199679861\n",
      "Progress:  0.4\n",
      "Negative likelihood:  0.24772567045947125\n",
      "Negative likelihood:  0.011009744360597287\n",
      "Negative likelihood:  0.19055836840545057\n",
      "Negative likelihood:  0.04784039176724185\n",
      "Progress:  0.5\n",
      "Negative likelihood:  0.18438684456737042\n",
      "Negative likelihood:  0.49428677305514\n",
      "Negative likelihood:  0.0742207882879798\n",
      "Negative likelihood:  0.21816911907234626\n",
      "Negative likelihood:  0.7839622608125828\n",
      "Progress:  0.6\n",
      "Negative likelihood:  1.36691581255367\n",
      "Negative likelihood:  0.1636264596954141\n",
      "Negative likelihood:  0.21936232555427962\n",
      "Negative likelihood:  3.3192238013405713\n",
      "Progress:  0.7\n",
      "Negative likelihood:  0.18480430892390076\n",
      "Negative likelihood:  0.1082348320173374\n",
      "Negative likelihood:  1.4624315763517028\n",
      "Negative likelihood:  0.683776790564164\n",
      "Negative likelihood:  0.03510909787106696\n",
      "Progress:  0.8\n",
      "Negative likelihood:  0.1663566535207969\n",
      "Negative likelihood:  0.05917443043627375\n",
      "Negative likelihood:  0.14233818364361733\n",
      "Negative likelihood:  0.1337904551921111\n",
      "Progress:  0.9\n",
      "Negative likelihood:  0.18404638565747206\n",
      "Negative likelihood:  0.26407375702615554\n",
      "Negative likelihood:  0.17022073068342408\n",
      "Negative likelihood:  0.23704436489339953\n",
      "Negative likelihood:  0.2635362521911931\n",
      "0.7255216852425433\n",
      "0.9385353714531874\n",
      "0.11255061580097799\n",
      "0.2384316566980278\n",
      "0.1292066705179405\n",
      "1.261214269347479\n",
      "1.0984464143186004\n",
      "0.34086700048740315\n",
      "0.18495569692617528\n",
      "0.0980116891800537\n",
      "0.1095225206954143\n",
      "3.658811707438858\n",
      "0.19078432609048473\n",
      "0.1374907231458898\n",
      "0.052483660809480887\n",
      "0.021056963482272203\n",
      "2.9247461773795655\n",
      "0.23300294187736514\n",
      "0.1909766671132867\n",
      "0.012465978019071728\n",
      "0.36885605481312633\n",
      "0.1625182398932596\n",
      "0.09182195158824996\n",
      "0.06513916697371393\n",
      "0.3755330922990082\n",
      "0.07269653615793759\n",
      "0.11194905306947256\n",
      "0.5131329541477632\n",
      "0.23200857754457258\n",
      "0.7625335521040016\n",
      "0.4607718946708069\n",
      "0.5295903057795278\n",
      "0.005210401477348923\n",
      "0.08064290527586293\n",
      "0.03961156224562681\n",
      "0.4368650366767031\n",
      "0.9600350849297484\n",
      "0.5422878855237417\n",
      "0.0006689072595732488\n",
      "0.0071609116209105095\n",
      "0.9422069031813309\n",
      "0.08212516969867428\n",
      "0.33685288286403986\n",
      "0.08236035614847723\n",
      "0.8231840666641862\n",
      "0.3510304575179755\n",
      "0.031349852415914085\n",
      "0.06731776221506093\n",
      "0.1110168009674357\n",
      "0.1397448915146664\n",
      "0.4549070812824013\n",
      "0.25836198006282035\n",
      "0.8447217478458272\n",
      "0.0013376059908810894\n",
      "0.1469319544221567\n",
      "0.018308640581516118\n",
      "0.17309599898317304\n",
      "0.00522818234275803\n",
      "0.20398104573470302\n",
      "0.19139545164400296\n",
      "0.1761040625613156\n",
      "0.06828914102985914\n",
      "0.36694036217199655\n",
      "0.2680093120636835\n",
      "0.0929836656171197\n",
      "0.08359270596693691\n",
      "0.1768738175009167\n",
      "0.005313196999807807\n",
      "0.13587372982608129\n",
      "0.24748840711070508\n",
      "0.05349103247145902\n",
      "0.00785934956485844\n",
      "0.17672319454497443\n",
      "0.16654628525205878\n",
      "0.08223692635583114\n",
      "0.3306304324415533\n",
      "0.007486982032274599\n",
      "0.4898379116097532\n",
      "0.10751942823229403\n",
      "0.12738402483194566\n",
      "0.1447592059806304\n",
      "0.01258327055744058\n",
      "0.32404438762852794\n",
      "0.061634150131906554\n",
      "0.15883952459213674\n",
      "2.1555076726413045\n",
      "0.4542203864128671\n",
      "0.17123060314281394\n",
      "0.18680166171671123\n",
      "0.02174178746317127\n",
      "0.13139218451680904\n",
      "0.22836824050900137\n",
      "0.014338449903351874\n",
      "0.08175761652676992\n",
      "0.005026458059681172\n",
      "0.08822497260573343\n",
      "0.22463957626682418\n",
      "0.12565589663308332\n",
      "0.1767793520914916\n",
      "0.2632275159866485\n",
      "0.2327330947387958\n",
      "0.23384644839626836\n",
      "0.31473074939819495\n",
      "0.5081574306271166\n",
      "0.5596042800203863\n",
      "0.3061412320811103\n",
      "4.3495972413334885\n",
      "0.001963168361808392\n",
      "0.2986390916115732\n",
      "0.000527117702255617\n",
      "0.7845514256152308\n",
      "0.24588641001834585\n",
      "4.168505504593249\n",
      "0.013882865872110014\n",
      "1.0255194173115834\n",
      "0.21206785939870504\n",
      "0.041912209748812476\n",
      "0.07651294642919731\n",
      "0.24142821816699186\n",
      "0.0012268588163174133\n",
      "0.12126518690178098\n",
      "0.7510809107599956\n",
      "0.22328933707751208\n",
      "0.01102651403042977\n",
      "0.23724541596624754\n",
      "1.1167708383161854\n",
      "0.30372778191993044\n",
      "0.2000322366199741\n",
      "5.127006362746008\n",
      "1.120641046878545\n",
      "0.08025587786106991\n",
      "0.1288968017450577\n",
      "0.09285218797851974\n",
      "0.14699075933264225\n",
      "0.2601323074878952\n",
      "0.0009196477499673611\n",
      "0.7126399505047462\n",
      "0.15634857582370326\n",
      "1.1763062995631832\n",
      "0.04699280994404169\n",
      "0.12489730384375883\n",
      "0.3874670539533692\n",
      "0.2920796668652863\n",
      "0.052996015975586896\n",
      "0.2910772799742567\n",
      "0.20218393670814078\n",
      "0.048656919469159766\n",
      "0.00011307114005696693\n",
      "0.037768725538756306\n",
      "0.18408011309119562\n",
      "0.5249141245664135\n",
      "0.23635174010249257\n",
      "0.17223215970212313\n",
      "0.003001834675235188\n",
      "0.2820343574528612\n",
      "0.12441794890253896\n",
      "0.14448078004027368\n",
      "0.3852346155535776\n",
      "0.121445524925186\n",
      "0.2663197389377264\n",
      "0.026244139327631255\n",
      "1.300822002852302\n",
      "0.17775827969485372\n",
      "0.20746887631524963\n",
      "0.14775987636336413\n",
      "0.1916841814357523\n",
      "0.2633671356114425\n",
      "0.1466691158788942\n",
      "0.04483602950177674\n",
      "0.2962023568872615\n",
      "0.03653763000246827\n",
      "0.11194256687072834\n",
      "0.3706410511281561\n",
      "0.1403483496086487\n",
      "0.0834407664445401\n",
      "0.02066522941453643\n",
      "0.010666568865080338\n",
      "0.6331558833107264\n",
      "0.32276347837278796\n",
      "0.40296738117409775\n",
      "0.3639505709179785\n",
      "0.8906951710813692\n",
      "0.19525332925768685\n",
      "0.10836071479182588\n",
      "0.1589062715833177\n",
      "0.21789490466126987\n",
      "0.05332448732953174\n",
      "0.12140825110546985\n",
      "0.1594688284791159\n",
      "0.09415802404490896\n",
      "0.001393299670375245\n",
      "0.021149378843142455\n",
      "0.3128507716275005\n",
      "0.08780611199679861\n",
      "0.24772567045947125\n",
      "0.011009744360597287\n",
      "0.19055836840545057\n",
      "0.04784039176724185\n",
      "0.18438684456737042\n",
      "0.49428677305514\n",
      "0.0742207882879798\n",
      "0.21816911907234626\n",
      "0.7839622608125828\n",
      "1.36691581255367\n",
      "0.1636264596954141\n",
      "0.21936232555427962\n",
      "3.3192238013405713\n",
      "0.18480430892390076\n",
      "0.1082348320173374\n",
      "1.4624315763517028\n",
      "0.683776790564164\n",
      "0.03510909787106696\n",
      "0.1663566535207969\n",
      "0.05917443043627375\n",
      "0.14233818364361733\n",
      "0.1337904551921111\n",
      "0.18404638565747206\n",
      "0.26407375702615554\n",
      "0.17022073068342408\n",
      "0.23704436489339953\n",
      "0.2635362521911931\n"
     ]
    }
   ],
   "source": [
    "filename = 'unlabeled-data.txt'\n",
    "fullsequence = loadData(filename)\n",
    "print (\"Full sequence loaded...\")\n",
    "print (\"Total unique words: \", len(uniqueWords))\n",
    "print(\"Preparing negative sampling table\")\n",
    "samplingTable = negativeSampleTable(fullsequence, uniqueWords, wordcounts)\n",
    "\n",
    "result = trainer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == '__main__':\n",
    "#     if len(sys.argv)==2:\n",
    "#         filename = sys.argv[1]\n",
    "#         #... load in the file, tokenize it and assign each token an index.\n",
    "#         #... the full sequence of characters is encoded in terms of their one-hot positions\n",
    "\n",
    "#         fullsequence= loadData(filename)\n",
    "#         print (\"Full sequence loaded...\")\n",
    "#         #print(uniqueWords)\n",
    "#         #print (len(uniqueWords))\n",
    "\n",
    "\n",
    "\n",
    "#         #... now generate the negative sampling table\n",
    "#         print (\"Total unique words: \", len(uniqueWords))\n",
    "#         print(\"Preparing negative sampling table\")\n",
    "#         samplingTable = negativeSampleTable(fullsequence, uniqueWords, wordcounts)\n",
    "\n",
    "\n",
    "#         #... we've got the word indices and the sampling table. Begin the training.\n",
    "#         #... NOTE: If you have already trained a model earlier, preload the results (set preload=True) (This would save you a lot of unnecessary time)\n",
    "#         #... If you just want to load an earlier model and NOT perform further training, comment out the train_vectors() line\n",
    "#         #... ... and uncomment the load_model() line\n",
    "\n",
    "#         #train_vectors(preload=False)\n",
    "#         [word_embeddings, proj_embeddings] = load_model()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         #... we've got the trained weight matrices. Now we can do some predictions\n",
    "#         targets = [\"good\", \"bad\", \"scary\", \"funny\"]\n",
    "#         for targ in targets:\n",
    "#             print(\"Target: \", targ)\n",
    "#             bestpreds= (prediction(targ))\n",
    "#             for pred in bestpreds:\n",
    "#                 print (pred[\"word\"],\":\",pred[\"score\"])\n",
    "#             print (\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "#         #... try an analogy task. The array should have three entries, A,B,C of the format: A is to B as C is to ?\n",
    "#         print (analogy([\"son\", \"daughter\", \"man\"]))\n",
    "#         print (analogy([\"thousand\", \"thousands\", \"hundred\"]))\n",
    "#         print (analogy([\"amusing\", \"fun\", \"scary\"]))\n",
    "#         print (analogy([\"terrible\", \"bad\", \"amazing\"]))\n",
    "\n",
    "\n",
    "\n",
    "#         #... try morphological task. Input is averages of vector combinations that use some morphological change.\n",
    "#         #... see how well it predicts the expected target word when using word_embeddings vs proj_embeddings in\n",
    "#         #... the morphology() function.\n",
    "\n",
    "#         s_suffix = [word_embeddings[wordcodes[\"stars\"]] - word_embeddings[wordcodes[\"star\"]]]\n",
    "#         others = [[\"types\", \"type\"],\n",
    "#                   [\"ships\", \"ship\"],\n",
    "#                   [\"values\", \"value\"],\n",
    "#                   [\"walls\", \"wall\"],\n",
    "#                   [\"spoilers\", \"spoiler\"]]\n",
    "#         for rec in others:\n",
    "#             s_suffix.append(word_embeddings[wordcodes[rec[0]]] - word_embeddings[wordcodes[rec[1]]])\n",
    "#         s_suffix = np.mean(s_suffix, axis=0)\n",
    "#         print (morphology([s_suffix, \"techniques\"]))\n",
    "#         print (morphology([s_suffix, \"sons\"]))\n",
    "#         print (morphology([s_suffix, \"secrets\"]))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     else:\n",
    "#         print (\"Please provide a valid input filename\")\n",
    "#         sys.exit()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
